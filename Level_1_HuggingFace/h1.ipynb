{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM's with Hugging face\n",
    "\n",
    "**Transformer Library Overview**\n",
    "**Model Inference**\n",
    "**Model Fine Tuning and Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tranformer Overview \n",
    "Special Tokens,  Examples\n",
    "Attention\n",
    "Task : Text classification, Token Classification\n",
    "\n",
    "https://huggingface.co/tasks \n",
    "https://huggingface.co/models\n",
    "https://huggingface.co/datasets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a Conditional Language Model?**\n",
    "\n",
    "Conditional Language Model â€“ also known as a Sequence-to-Sequence (Seq2Seq) model. **These models have an encoder-decoder architecture:**\n",
    "\n",
    "The encoder processes the input sequence (e.g., a document, a question) and transforms it into a hidden representation.\n",
    "\n",
    "The decoder then generates the output sequence (e.g., a summary, an answer), conditioned on that hidden state.\n",
    "\n",
    "I used models like T5 and BART, which are pretrained for tasks like summarization, translation, and Q&A. We used Hugging Faceâ€™s AutoTokenizer and AutoModelForSeq2SeqLM to simplify integration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usecase : \n",
    "\"In a recent project, when team was building an AI assistant that could not only answer user questions but also translate and summarize domain-specific content in real-time. **The challenge was to generate output based on a specific input â€“ meaning the output wasnâ€™t just a continuation of input like in GPT models, but needed to be conditioned on the input itself.\"**\n",
    "\n",
    "task was to select and implement a model architecture that could handle such conditional tasks â€“ where output depends explicitly on the input, like translation (English â†’ French), summarization, or question answering. It had to support sequence-to-sequence transformation and be fine-tunable for our domain.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/akv/Documents/hfc-works-ashok/hfc-works-ashok/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le temps est agrÃ©able aujourd'hui.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "#AutoTokenizer is a factory class that can automatically load the tokenizer corresponding to a pre-trained model we specify\n",
    "#  (in this case, t5-small model).\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "#Next, we create an instance of the AutoTokenizer class by calling the from_pretrained method. This tokenizer is designed to work with the t5-small model \n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "\n",
    "input_text = \"translate English to French: The weather is nice today.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**\"In Short , Conditional Language Models (Seq2Seq) are used when the output is directly dependent on a specific input â€“ unlike causal models which generate freely. They're ideal for summarization, translation, and question answering tasks where understanding and transforming input sequences is critical.\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-base and revision a9723ea (https://huggingface.co/google-t5/t5-base).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'translation_text': \"Je m'appelle Mike et j'aime l'apprentissage par machine\"}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "model = pipeline(task=\"translation_en_to_fr\")\n",
    "answer = model(\"My name is Mike and i love machine learning\")\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Casual Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Casual Language Models ,When to use them ?\n",
    "\"Causal Language Models are designed for autoregressive tasks â€” where the model generates the next word/token based only on prior context. Theyâ€™re ideal for chatbots, code assistants, and story generation. Unlike seq2seq models, they donâ€™t require an encoder-decoder architecture â€” the decoder itself learns to generate outputs step-by-step.\"\n",
    "\n",
    "### When to use them ?\n",
    "\"In  GenAI projects, we were developing an AI writing assistant that could autocomplete user input, generate stories, and provide intelligent code suggestions. The core requirement was a model that could predict the next token in a sequence given only the previous context â€” no peeking ahead.\"\n",
    "\n",
    "\"My task was to select and fine-tune a language model that could handle next-token prediction effectively. This was essential for features like autocomplete, chat generation, and coding assistance where the model builds output word-by-word based only on what has already been said or written.\n",
    "\n",
    "So chose a Causal Language Model (CLM) â€” specifically GPT-2 â€” because itâ€™s designed to predict the next token in a sequence, given all previous tokens. CLMs use a unidirectional attention mechanism, meaning each word sees only past words during training and inference, which is ideal for autoregressive generation tasks.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "prompt = \"Once upon a time\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
    "\n",
    "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Masked Language Models, when to use them?\n",
    "\n",
    "\"Masked Language Models (MLMs) like BERT are pre-trained by masking random words in a sentence and predicting them using full bidirectional context. This makes them excellent for understanding tasks â€” such as classification, QA, and information extraction â€” where deep contextual understanding is key. Unlike causal models, MLMs donâ€™t generate new text but understand existing text with high accuracy.\"\n",
    "\n",
    "**When to use Masked Language model**\n",
    "\"In one of our NLP pipeline projects, we needed to build a system that could understand the structure and meaning of customer feedback â€” classify sentiment, identify key entities, and detect intent. For this, we needed a model that had a deep understanding of context â€” not just generating text but analyzing it.\"\n",
    "\n",
    "task was to select a language model that could learn bidirectional context from text. It had to understand not only the words that come before a token, but also the words that follow it. This was important for tasks like sentiment analysis and named entity recognition, where both past and future context matter.\"\n",
    "\n",
    "Chose a Masked Language Model (MLM) â€” specifically BERT (Bidirectional Encoder Representations from Transformers) â€” because it is trained to predict missing tokens in a sentence by looking at both directions (left and right context)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Œ Prediction: paris\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "sentence = \"The capital of France is [MASK].\"\n",
    "inputs = tokenizer(sentence, return_tensors=\"pt\")\n",
    "mask_token_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "predicted_token_id = logits[0, mask_token_index].argmax(axis=-1)\n",
    "predicted_word = tokenizer.decode(predicted_token_id)\n",
    "print(f\"ðŸ“Œ Prediction: {predicted_word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huggingface Libraries (most used are)\n",
    "\n",
    "Transformers \n",
    "Provides pretrained models for NLP, computer vision, speech, and multimodal tasks.\n",
    "Supports 1000+ models (like BERT, GPT-2, T5, BART, LLaMA, Whisper, etc.)\n",
    "https://github.com/huggingface/transformers\n",
    "https://huggingface.co/docs/transformers/index\n",
    "\n",
    "\n",
    "\n",
    "Accelerate : Accelerate is a library that enables the same PyTorch code to be run across any distributed configuration by adding just four lines of code!https://github.com/huggingface/accelerate\n",
    "https://huggingface.co/docs/accelerate/index\n",
    "\n",
    "Evaluate: A library for easily evaluating machine learning models and datasets.\n",
    "https://huggingface.co/docs/evaluate/index\n",
    "https://github.com/huggingface/evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging face Transformer Library - \n",
    "AutoModel, AutoTokenizer, AutoModelForSequenceClassification, etc.\n",
    "\n",
    "Can be used for Task-specific models: summarization, translation, text generation, etc.\n",
    "\n",
    "Supports both PyTorch and TensorFlow.\n",
    "\n",
    "Install pip and python\n",
    "pip install torch\n",
    "pip install transformers\n",
    "\n",
    "\n",
    "**Using Transformers Pipelines:**\n",
    "Pipelines are the easiest way yo start using huggingface transformers for inference, Pipelines abstract most of the complex code required to run the model, You can use pipeline to run different tasks eg: text classification, SENTIMENTAL Analysis etc..,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "model = pipeline(task=\"sentiment-analysis\")\n",
    "#The above cell will download a model for sentiment analysis  \n",
    "#As you can see from the printed output, since we didnâ€™t specify a model, the library downloads the current default model for the sentiment-analysis task, \n",
    "# which is the distilbert-base-uncased-finetuned-sst-2-english model. \n",
    "# The output also says that itâ€™s not recommended to not specify a model to use, because the default models for the tasks may vary over time.\n",
    "model = pipeline(model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "answer = model(\"The weather is great today\")\n",
    "print(answer)\n",
    "#The model classified the input sentence with the â€œPOSITIVEâ€ class, with a score of ~0.99. We can also pass several input sentences to the model as a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Auto Classess:\n",
    "AutoTokenizer also allows you to automatically load the tokenizer for any specific model \n",
    "\n",
    "The AutoModel automatically infers and loads the correct transfoermer architecture for a give mode\n",
    "\n",
    "There are several AutoModel Clasess per task: \n",
    "AutoModelForSeq2SeqLM -- class Used to Load conditional Language models\n",
    "https://huggingface.co/docs/transformers/model_doc/auto#transformers.TFAutoModelForSeq2SeqLM\n",
    "\n",
    "\n",
    "AutoModelForCausalLM  - class Used to Load Casual language models\n",
    "https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM\n",
    "\n",
    "AutoModelForSequenceClassification --class used to load classification models\n",
    "\n",
    "\n",
    " \"I was working on a project to build a domain-aware chatbot that answers questions from a medical knowledge base. Since we needed to use a pretrained language model for encoding inputs and generating responses, the team decided to use Hugging Face Transformers due to its ease of integration and model flexibility.\"\n",
    "\n",
    " Your task was to efficiently load a Hugging Face model with minimal boilerplate, allowing dynamic switching between models like BERT, RoBERTa, or GPT-2 without changing much of the code. This also had to include proper tokenization of inputs to match the selected model.\"\n",
    "\n",
    " \"To make the code model-agnostic and dynamic, I used AutoTokenizer and AutoModel from the Hugging Face transformers library. These classes automatically load the correct tokenizer and model architecture based on the model name or path.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Step 1: Load the tokenizer and model\n",
    "# model_name = \"t5-small\"\n",
    "model_name = \"facebook/bart-large-cnn\"\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) # \tAutomatically loads the correct tokenizer for the model\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name) # Loads a seq2seq model like T5 or BART\n",
    "\n",
    "# Step 2: Prepare input text (with a task prefix)\n",
    "input_text = \"summarize: Transformers models by Hugging Face allow users to easily run state-of-the-art NLP models for tasks like summarization, translation, and question answering.\"\n",
    "#Converts input text to model-readable tokens\n",
    "\n",
    "\n",
    "# Step 3: Tokenize the input\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Step 4: Perform inference (generate output) # Generates prediction from the input tokens\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=50,\n",
    "    num_beams=4,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "# Step 5: Decode the generated tokens to text # Converts the output tokens back to human-readable text\n",
    "summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"ðŸ“Œ Summary:\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**AutoTokenizer**\n",
    "\n",
    "### AutoTokenizer is a class that automatically selects and loads the appropriate tokenizer for a given pre-trained model, simplifying the process of preparing text for model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', '.']\n",
      "[1188, 1110, 170, 6876, 5650, 119]\n",
      "This is a sample sentence.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "# tokenizer(\"Using a Transformer network is simple\")\n",
    "# Tokenize a sentence\n",
    "text = \"This is a sample sentence.\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(tokens)\n",
    "\n",
    "    # Convert tokens to IDs\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "print(ids)\n",
    "\n",
    "    # Decode IDs back to text\n",
    "decoded_text = tokenizer.decode(ids)\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Automodel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5ForConditionalGeneration(\n",
      "  (shared): Embedding(32128, 512)\n",
      "  (encoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder): T5Stack(\n",
      "    (embed_tokens): Embedding(32128, 512)\n",
      "    (block): ModuleList(\n",
      "      (0): T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (relative_attention_bias): Embedding(32, 8)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1-5): 5 x T5Block(\n",
      "        (layer): ModuleList(\n",
      "          (0): T5LayerSelfAttention(\n",
      "            (SelfAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (1): T5LayerCrossAttention(\n",
      "            (EncDecAttention): T5Attention(\n",
      "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
      "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "          (2): T5LayerFF(\n",
      "            (DenseReluDense): T5DenseActDense(\n",
      "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
      "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "              (act): ReLU()\n",
      "            )\n",
      "            (layer_norm): T5LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (final_layer_norm): T5LayerNorm()\n",
      "    (dropout): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Fine-Tuning\n",
    "Training acomplex model on limited data can lead to overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging face team created several scripts to train/fine-tune models on several tasks . The scripts can be found -- https://github.com/huggingface/transformers/tree/main/examples/pytorch\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py -- Use to train/fine-tune for casual language models\n",
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py -- Use to train/fine-tune masked language models\n",
    "https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_classification.py -- use to tain/fine-tune text classfication models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following example fine-tunes GPT-2 on WikiText-2. We're using the raw WikiText-2 (no tokens were replaced before the tokenization). The loss here is that of causal language modeling.\n",
    "\n",
    "python run_clm.py \\\n",
    "    --model_name_or_path openai-community/gpt2 \\\n",
    "    --dataset_name wikitext \\\n",
    "    --dataset_config_name wikitext-2-raw-v1 \\\n",
    "    --per_device_train_batch_size 8 \\\n",
    "    --per_device_eval_batch_size 8 \\\n",
    "    --do_train \\\n",
    "    --do_eval \\\n",
    "    --output_dir /tmp/test-clm\n",
    "\n",
    "\n",
    "Parameter\tDescription\n",
    "run_clm.py\t--> Script from Hugging Face ðŸ¤— Transformers examples repo used for causal language modeling (like GPT-2).\n",
    "--model_name_or_path openai-community/gpt2\t--> Load the pretrained GPT-2 model from the Hugging Face hub. You can also use a local directory here if you've downloaded or fine-tuned a model.\n",
    "--dataset_name wikitext\t  --> Use the wikitext dataset from the Hugging Face datasets library.\n",
    "--dataset_config_name wikitext-2-raw-v1\t--> Choose a specific configuration: wikitext-2-raw-v1 means using the raw version of WikiText-2 (no <unk> or token replacements).\n",
    "--per_device_train_batch_size 8\t--> Train with a batch size of 8 per GPU. If you're using 2 GPUs, effective batch size is 16.\n",
    "--per_device_eval_batch_size 8 --> \tEvaluate with batch size of 8 per GPU during validation.\n",
    "--do_train\t--> Perform training phase.\n",
    "--do_eval\t--> Evaluate the model during training.\n",
    "--output_dir /tmp/test-clm\tDirectory where the model checkpoints, logs, and metrics will be saved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are Tokenizers?\n",
    "Imagine that youâ€™re trying to teach a robot to understand and speak human languages. The first challenge youâ€™d face is how to break down language into pieces the robot can digest. Thatâ€™s where tokenizers come in.\n",
    "\n",
    "Tokenizers dissect complex language into manageable pieces, transforming raw text into a structured form that AI models can easily process. This seemingly simple step is crucial, enabling machines to grasp the nuances of human communication.\n",
    "\n",
    "Think of tokenizers as the chefs who chop ingredients before a meal is cooked. Without this step, preparing complex dishes (or understanding complex sentences) would be much harder.\n",
    "\n",
    "Through tokenization, AI systems can recognize patterns, understand context, and generate responses that are increasingly similar to human interaction.\n",
    "\n",
    "By breaking down the complexities of language into digestible bits, tokenizers not only enhance AIâ€™s linguistic capabilities but also pave the way for more intuitive, efficient, and accurate machine learning models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your Understanding :\n",
    "\n",
    "1) What is a language model, and how is it evaluated?\n",
    "2) What are the primary differences between Hugging Face\n",
    "Transformers, Datasets, and Tokenizers libraries, and how do they\n",
    "integrate to streamline NLP workflows?\n",
    "3) Describe how to use Hugging Face Pipelines for end-to-end\n",
    "inference. What types of NLP tasks can pipelines handle, and what are\n",
    "the main advantages of using them?\n",
    "4) How does Hugging Face's Accelerate library improve model training,\n",
    "and what challenges does it address in scaling NLP models across\n",
    "different hardware setups?\n",
    "5) How does Hugging Face's transformers library facilitate transfer\n",
    "learning, and what are the typical steps for fine-tuning a pre-trained\n",
    "model on a custom dataset?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
